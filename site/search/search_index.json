{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"What is instructor-ai?","text":"<p>Instructor makes it easy to get structured data like JSON from LLMs like GPT-3.5, GPT-4, GPT-4-Vision, and open-source models including Mistral/Mixtral, Anyscale, Ollama, and llama-cpp-python.</p> <p>Our library is currently in active development and we're looking for active contributors interested in contributing!</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>To install <code>instructor-ai</code>, you'll need to add the following to your cargo.toml file</p> <pre><code>instructor-ai = \"0.1.0\"\ninstruct-macros = \"0.1.1\"\nopenai-api-rs = \"4.1.0\"\nserde = { version = \"1.0\", features = [\"derive\"] }\nserde_json = \"1.0\"\ninstruct-macros-types = \"0.1.2\"\n</code></pre> <p>Getting started with structured extraction is then as simple as declaring a new struct with the <code>InstructMacro</code> and importing the <code>ParamterInfo</code> and <code>StructInfo</code> types.</p> <pre><code>use std::env;\nuse instruct_macros::InstructMacro;\nuse instruct_macros_types::{ParameterInfo, StructInfo};\nuse instructor_ai::from_openai;\nuse openai_api_rs::v1::{\n    api::Client,\n    chat_completion::{self, ChatCompletionRequest},\n    common::GPT3_5_TURBO,\n};\nuse serde::{Deserialize, Serialize};\n\nfn main() {\n    let client = Client::new(env::var(\"OPENAI_API_KEY\").unwrap().to_string());\n    let instructor_client = from_openai(client);\n\n    #[derive(InstructMacro, Debug, Serialize, Deserialize)]\n    // This represents a single user\n    struct UserInfo {\n        // This represents the name of the user\n        name: String,\n        // This represents the age of the user\n        age: u8,\n    }\n\n    let req = ChatCompletionRequest::new(\n        GPT3_5_TURBO.to_string(),\n        vec![chat_completion::ChatCompletionMessage {\n            role: chat_completion::MessageRole::user,\n            content: chat_completion::Content::Text(String::from(\n                \"John Doe is a 30 year old software engineer\",\n            )),\n            name: None,\n        }],\n    );\n\n    let result = instructor_client\n        .chat_completion::&lt;UserInfo&gt;(req, 3)\n        .unwrap();\n\n    println!(\"{}\", result.name); // John Doe\n    println!(\"{}\", result.age); // 30\n}\n\n</code></pre>"}]}